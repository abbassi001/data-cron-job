#!/bin/bash

# ============================================================
# SCRIPT PRINCIPAL - ORCHESTRATEUR DE TOUT LE PROCESSUS
# ============================================================
# Ce script unique lance l'ensemble du processus :
# 1. Configuration initiale
# 2. T√©l√©chargement massif de donn√©es (environ 250MB)
# 3. Traitement et analyse avanc√©e des donn√©es
# 4. G√©n√©ration de rapports avec visualisations am√©lior√©es
# 5. Versionning Git
# 6. Envoi de notifications Discord avec graphiques int√©gr√©s
# ============================================================

# Fonction pour afficher un texte en figlet si disponible
show_figlet() {
    local text="$1"
    if command -v figlet &> /dev/null; then
        figlet -f standard "$text"
    else
        echo ""
        echo "=== $text ==="
        echo ""
    fi
}

# Afficher le titre du projet
show_figlet "Data Process"
echo "============================================================"
echo "  SYST√àME AVANC√â DE TRAITEMENT DE DONN√âES MASSIVES"
echo "============================================================"
echo ""

# === CONFIGURATION ===
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
DATA_DIR="$PROJECT_DIR/data"
RAW_DIR="$DATA_DIR/raw"
PROCESSED_DIR="$DATA_DIR/processed"
REPORT_DIR="$DATA_DIR/reports"
VISUALIZATION_DIR="$DATA_DIR/visualizations"
LOG_DIR="$PROJECT_DIR/logs"
DATE=$(date +%Y-%m-%d)
LOG_FILE="$LOG_DIR/complet_$DATE.log"

# Configuration Discord - URL du webhook
DISCORD_WEBHOOK="https://discord.com/api/webhooks/1369668625744662669/Vj-FfURhiuzXR7qD_kXIaw8oAl_-A41L8spsGnCdAZ2IKYSVgeXHeJ4f_YDA2at7-cC0"

# V√©rifier que le script est ex√©cut√© depuis le r√©pertoire du projet
cd "$PROJECT_DIR" || {
  echo "Erreur: Impossible d'acc√©der au r√©pertoire du projet: $PROJECT_DIR"
  exit 1
}

# === INITIALISATION ===
# Cr√©er les r√©pertoires n√©cessaires
mkdir -p "$RAW_DIR" "$PROCESSED_DIR" "$REPORT_DIR" "$VISUALIZATION_DIR" "$LOG_DIR"

# Initialiser le journal
echo "===== D√âBUT DU PROCESSUS COMPLET: $(date '+%Y-%m-%d %H:%M:%S') =====" | tee -a "$LOG_FILE"
echo "üë®‚Äçüíª Script lanc√© par: $(whoami)" | tee -a "$LOG_FILE"
echo "üìÇ R√©pertoire du projet: $PROJECT_DIR" | tee -a "$LOG_FILE"

# V√©rification de l'environnement
show_figlet "Check Env"
echo "--- V√©rification de l'environnement ---" | tee -a "$LOG_FILE"
if command -v python3 &> /dev/null; then
    PYTHON_VERSION=$(python3 --version)
    echo "‚úÖ Python install√©: $PYTHON_VERSION" | tee -a "$LOG_FILE"
else
    echo "‚ùå Python non install√©. Installation requise." | tee -a "$LOG_FILE"
    exit 1
fi

if command -v git &> /dev/null; then
    GIT_VERSION=$(git --version)
    echo "‚úÖ Git install√©: $GIT_VERSION" | tee -a "$LOG_FILE"
    GIT_ENABLED=true
    GIT_BRANCH="data-updates"
else
    echo "‚ö†Ô∏è Git non install√©. Le versionning ne sera pas disponible." | tee -a "$LOG_FILE"
    GIT_ENABLED=false
fi

# V√©rifier les d√©pendances Python
echo "--- V√©rification des d√©pendances Python ---" | tee -a "$LOG_FILE"
python3 -c "import pandas" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "‚ùå Pandas non install√©. Installation en cours..." | tee -a "$LOG_FILE"
    pip install pandas || {
        echo "‚ùå √âchec de l'installation de pandas. Veuillez l'installer manuellement." | tee -a "$LOG_FILE"
        exit 1
    }
else
    echo "‚úÖ Pandas install√©" | tee -a "$LOG_FILE"
fi

python3 -c "import matplotlib" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "‚ùå Matplotlib non install√©. Installation en cours..." | tee -a "$LOG_FILE"
    pip install matplotlib || {
        echo "‚ùå √âchec de l'installation de matplotlib. Veuillez l'installer manuellement." | tee -a "$LOG_FILE"
        exit 1
    }
else
    echo "‚úÖ Matplotlib install√©" | tee -a "$LOG_FILE"
fi

python3 -c "import seaborn" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "‚ùå Seaborn non install√©. Installation en cours..." | tee -a "$LOG_FILE"
    pip install seaborn || {
        echo "‚ùå √âchec de l'installation de seaborn. Les visualisations seront limit√©es." | tee -a "$LOG_FILE"
    }
else
    echo "‚úÖ Seaborn install√©" | tee -a "$LOG_FILE"
fi

python3 -c "import scipy" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "‚ùå SciPy non install√©. Installation en cours..." | tee -a "$LOG_FILE"
    pip install scipy || {
        echo "‚ùå √âchec de l'installation de scipy. Certaines analyses statistiques seront limit√©es." | tee -a "$LOG_FILE"
    }
else
    echo "‚úÖ SciPy install√©" | tee -a "$LOG_FILE"
fi

# V√©rifier requests pour Discord
python3 -c "import requests" 2>/dev/null
if [ $? -ne 0 ]; then
    echo "‚ùå Requests non install√©. Installation en cours..." | tee -a "$LOG_FILE"
    pip install requests || {
        echo "‚ùå √âchec de l'installation de requests. Les notifications Discord ne fonctionneront pas." | tee -a "$LOG_FILE"
    }
else
    echo "‚úÖ Requests install√©" | tee -a "$LOG_FILE"
fi

# V√©rifier unzip pour les archives
if ! command -v unzip &> /dev/null; then
    echo "‚ö†Ô∏è Unzip n'est pas install√©. L'extraction des archives ZIP peut √©chouer." | tee -a "$LOG_FILE"
    echo "‚ö†Ô∏è Pour installer unzip : sudo apt-get install unzip (Debian/Ubuntu)" | tee -a "$LOG_FILE"
fi

# V√©rifier figlet (optionnel)
if ! command -v figlet &> /dev/null; then
    echo "‚ÑπÔ∏è Figlet n'est pas install√©. Les banni√®res seront simplifi√©es." | tee -a "$LOG_FILE"
    echo "‚ÑπÔ∏è Pour installer figlet : sudo apt-get install figlet (Debian/Ubuntu)" | tee -a "$LOG_FILE"
fi

# === FONCTIONS UTILITAIRES ===
# Script Python pour envoyer des notifications Discord avec graphiques
create_discord_charts_script() {
    local DISCORD_SCRIPT="$SCRIPT_DIR/send_discord_with_charts.py"
    
    cat > "$DISCORD_SCRIPT" << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import json
import requests
from datetime import datetime
from glob import glob

def read_report_content(report_path):
    """
    Lit le contenu d'un rapport HTML et extrait les √©l√©ments cl√©s
    """
    try:
        with open(report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # Extraire les informations pertinentes
        # Ceci est une version simplifi√©e - vous pourriez avoir besoin d'un parsing HTML plus avanc√©
        summary = []
        
        # Extraire le nombre de fichiers trait√©s
        if 'Nombre de fichiers trait√©s:' in content:
            start = content.find('Nombre de fichiers trait√©s:')
            end = content.find('</p>', start)
            if start > 0 and end > start:
                file_count = content[start:end].split(':')[1].strip()
                summary.append(f"Fichiers trait√©s: {file_count}")
        
        # Extraire les graphiques g√©n√©r√©s
        if '<h2>Graphiques g√©n√©r√©s</h2>' in content:
            start = content.find('<h2>Graphiques g√©n√©r√©s</h2>')
            list_start = content.find('<ul>', start)
            list_end = content.find('</ul>', list_start)
            
            if list_start > 0 and list_end > list_start:
                graphs_list = content[list_start:list_end]
                graphs = []
                
                start_idx = 0
                while True:
                    li_start = graphs_list.find('<li>', start_idx)
                    if li_start == -1:
                        break
                    li_end = graphs_list.find('</li>', li_start)
                    if li_end == -1:
                        break
                    
                    graph_name = graphs_list[li_start+4:li_end].strip()
                    graphs.append(graph_name)
                    start_idx = li_end
                
                if graphs:
                    summary.append(f"Graphiques: {', '.join(graphs)}")
        
        # Si on n'a pas pu extraire d'infos, retourner un message par d√©faut
        if not summary:
            return "Rapport HTML g√©n√©r√©. Consultez le fichier pour plus de d√©tails."
        
        return "\n".join(summary)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la lecture du rapport: {str(e)}")
        return "Rapport HTML g√©n√©r√©, mais impossible d'extraire le contenu."

def find_charts(report_dir, date_str):
    """
    Recherche les graphiques g√©n√©r√©s dans le r√©pertoire des rapports
    """
    # Recherche les fichiers PNG qui contiennent "chart" dans le nom
    charts = glob(os.path.join(report_dir, "*_chart.png"))
    
    # Si aucun graphique trouv√© dans les charts, chercher dans le dossier visualizations
    if not charts:
        viz_dir = os.path.join(os.path.dirname(report_dir), "visualizations")
        if os.path.exists(viz_dir):
            charts = glob(os.path.join(viz_dir, "*.png"))
    
    # Trouver aussi les dashboards
    dashboards = glob(os.path.join(os.path.dirname(report_dir), "visualizations", "*_dashboard.png"))
    if dashboards:
        # Privil√©gier les tableaux de bord s'ils existent
        return dashboards[0]
    
    # Filtrer par date si n√©cessaire
    if date_str and charts:
        today_charts = [c for c in charts if os.path.getmtime(c) > (datetime.now().timestamp() - 86400)]
        if today_charts:
            return today_charts[0]
    
    return charts[0] if charts else None

def upload_image_to_discord(webhook_url, image_path):
    """
    T√©l√©charge une image sur Discord en utilisant le webhook
    """
    try:
        print(f"üì§ Tentative d'envoi de l'image {image_path} vers Discord...")
        
        with open(image_path, 'rb') as img:
            # Utiliser la partie file pour envoyer l'image
            files = {'file': (os.path.basename(image_path), img, 'image/png')}
            
            response = requests.post(webhook_url, files=files)
            
        if response.status_code == 200:
            print(f"‚úÖ Image {os.path.basename(image_path)} envoy√©e avec succ√®s!")
            image_url = response.json().get('attachments', [{}])[0].get('url', '')
            return image_url
        else:
            print(f"‚ùå √âchec de l'envoi de l'image: {response.status_code}")
            print(response.text)
            return None
    
    except Exception as e:
        print(f"‚ùå Exception lors de l'envoi de l'image: {str(e)}")
        return None

def send_discord_message(webhook_url, message, title=None, report_path=None, chart_path=None):
    """
    Envoie un message √† Discord via un webhook, avec un r√©sum√© du rapport si disponible
    
    Args:
        webhook_url (str): URL du webhook Discord
        message (str): Le message √† envoyer
        title (str, optional): Titre du message (embeds)
        report_path (str, optional): Chemin vers le rapport HTML
        chart_path (str, optional): Chemin vers un graphique √† inclure
    """
    # Tenter d'abord d'envoyer l'image si sp√©cifi√©e
    image_url = None
    if chart_path and os.path.exists(chart_path):
        try:
            image_url = upload_image_to_discord(webhook_url, chart_path)
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible d'envoyer l'image: {str(e)}")
    
    # Pr√©parer le payload de base
    payload = {
        "content": message,
        "embeds": []
    }
    
    # Ajouter un embed avec titre et rapport si sp√©cifi√©
    if title:
        embed = {
            "title": title,
            "description": message,
            "color": 3447003,  # Bleu Discord
            "timestamp": datetime.now().isoformat(),
            "fields": []
        }
        
        # Si un rapport est sp√©cifi√© et existe
        if report_path and os.path.exists(report_path):
            report_content = read_report_content(report_path)
            
            embed["fields"].append({
                "name": "R√©sum√© du rapport",
                "value": report_content
            })
            
            # Ajouter le chemin du rapport pour r√©f√©rence
            embed["footer"] = {
                "text": f"Rapport complet: {os.path.basename(report_path)}"
            }
        
        # Si on a une URL d'image, l'ajouter √† l'embed
        if image_url:
            embed["image"] = {
                "url": image_url
            }
        
        payload["embeds"].append(embed)
    
    # Envoyer la requ√™te
    try:
        response = requests.post(
            webhook_url,
            data=json.dumps(payload),
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 204:
            print(f"‚úÖ Message Discord envoy√© avec succ√®s!")
            return True
        else:
            print(f"‚ùå Erreur lors de l'envoi du message Discord: {response.status_code}")
            print(response.text)
            return False
    
    except Exception as e:
        print(f"‚ùå Exception lors de l'envoi du message Discord: {str(e)}")
        return False

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python3 send_discord_with_charts.py webhook_url message [title] [report_path] [chart_path]")
        sys.exit(1)
    
    webhook_url = sys.argv[1]
    message = sys.argv[2]
    title = sys.argv[3] if len(sys.argv) > 3 else None
    report_path = sys.argv[4] if len(sys.argv) > 4 else None
    chart_path = sys.argv[5] if len(sys.argv) > 5 else None
    
    # Si aucun graphique n'est sp√©cifi√© mais qu'on a un r√©pertoire de rapport,
    # chercher automatiquement le premier graphique disponible
    if not chart_path and report_path:
        report_dir = os.path.dirname(report_path)
        date_str = datetime.now().strftime("%Y-%m-%d")
        chart_path = find_charts(report_dir, date_str)
        if chart_path:
            print(f"üîç Graphique trouv√© automatiquement: {chart_path}")
    
    success = send_discord_message(webhook_url, message, title, report_path, chart_path)
    sys.exit(0 if success else 1)
EOF
    
    chmod +x "$DISCORD_SCRIPT"
    echo "‚úÖ Script d'envoi Discord avec graphiques cr√©√©: $DISCORD_SCRIPT" | tee -a "$LOG_FILE"
}

# Script Python pour le traitement avanc√© des donn√©es
create_processing_script() {
    local PROCESSING_SCRIPT="$SCRIPT_DIR/process_data_advanced.py"
    
    echo "üìù Cr√©ation du script de traitement avanc√© des donn√©es..." | tee -a "$LOG_FILE"
    
    cat > "$PROCESSING_SCRIPT" << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script avanc√© de traitement des donn√©es avec visualisations am√©lior√©es
"""

import os
import sys
import json
import logging
import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from pathlib import Path
from matplotlib.gridspec import GridSpec
from matplotlib.ticker import MaxNLocator
import matplotlib.ticker as mtick
from scipy import stats
import gc

# Configuration am√©lior√©e de matplotlib
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.titlesize'] = 16
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
plt.rcParams['legend.fontsize'] = 12
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['savefig.bbox'] = 'tight'

# Configuration
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
REPORT_DIR = DATA_DIR / "reports"
VISUALIZATION_DIR = DATA_DIR / "visualizations"
LOG_DIR = BASE_DIR / "logs"

# Configuration du logging
today = datetime.datetime.now().strftime("%Y-%m-%d")
log_file = LOG_DIR / f"enhanced_process_{today}.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Cr√©er les r√©pertoires n√©cessaires
os.makedirs(PROCESSED_DIR, exist_ok=True)
os.makedirs(REPORT_DIR, exist_ok=True)
os.makedirs(VISUALIZATION_DIR, exist_ok=True)

def find_files_to_process():
    """
    R√©cup√®re tous les fichiers √† traiter, y compris les fichiers extraits des ZIPs.
    """
    files_to_process = {}
    
    # Rechercher les fichiers CSV du jour
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    
    # Fichiers CSV directement t√©l√©charg√©s
    for file in RAW_DIR.glob(f"*_{today}.csv"):
        prefix = file.name.split("_")[0]
        files_to_process[prefix] = file
    
    # Fichiers extraits des ZIP
    for dir_path in RAW_DIR.glob(f"*_{today}_extracted"):
        prefix = dir_path.name.split("_")[0]
        
        # Trouver tous les CSV dans le r√©pertoire extrait
        extracted_csvs = list(dir_path.glob("**/*.csv"))
        
        if extracted_csvs:
            # Si plusieurs CSV, les traiter tous avec un pr√©fixe diff√©rent
            for i, csv_file in enumerate(extracted_csvs):
                sub_prefix = f"{prefix}_{i+1}"
                files_to_process[sub_prefix] = csv_file
    
    logger.info(f"Fichiers √† traiter: {len(files_to_process)} fichiers trouv√©s")
    for k, v in files_to_process.items():
        logger.info(f"  - {k}: {v}")
    
    return files_to_process

def optimize_dataframe(df):
    """
    Optimisation de la m√©moire utilis√©e par le DataFrame
    """
    start_mem = df.memory_usage().sum() / 1024**2
    logger.info(f"M√©moire utilis√©e avant optimisation: {start_mem:.2f} MB")
    
    for col in df.columns:
        # Conversion des colonnes enti√®res
        if pd.api.types.is_integer_dtype(df[col]):
            min_val = df[col].min()
            max_val = df[col].max()
            
            # Conversion au type entier le plus petit possible
            if min_val >= 0:
                if max_val < 255:
                    df[col] = df[col].astype(np.uint8)
                elif max_val < 65535:
                    df[col] = df[col].astype(np.uint16)
                elif max_val < 4294967295:
                    df[col] = df[col].astype(np.uint32)
            else:
                if min_val > -128 and max_val < 127:
                    df[col] = df[col].astype(np.int8)
                elif min_val > -32768 and max_val < 32767:
                    df[col] = df[col].astype(np.int16)
                elif min_val > -2147483648 and max_val < 2147483647:
                    df[col] = df[col].astype(np.int32)
        
        # Conversion des colonnes flottantes
        elif pd.api.types.is_float_dtype(df[col]):
            df[col] = df[col].astype(np.float32)
        
        # Conversion des colonnes cat√©gorielles
        elif pd.api.types.is_object_dtype(df[col]):
            if df[col].nunique() / len(df) < 0.5:  # Si moins de 50% de valeurs uniques
                df[col] = df[col].astype('category')
    
    end_mem = df.memory_usage().sum() / 1024**2
    logger.info(f"M√©moire utilis√©e apr√®s optimisation: {end_mem:.2f} MB")
    logger.info(f"R√©duction: {100 * (start_mem - end_mem) / start_mem:.2f}%")
    
    return df

def process_csv(file_path, output_prefix, max_rows=None):
    """
    Traite un fichier CSV avec des fonctionnalit√©s avanc√©es
    """
    try:
        logger.info(f"Traitement du fichier CSV: {file_path}")
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        logger.info(f"Taille du fichier: {file_size_mb:.2f} MB")
        
        # Pour les gros fichiers, utiliser un √©chantillon pour d√©tection du s√©parateur
        sample_size = min(1000, os.path.getsize(file_path))
        with open(file_path, 'r', errors='ignore') as f:
            sample = f.read(sample_size)
        
        # D√©tection intelligente du s√©parateur
        separators = [',', ';', '\t', '|']
        sep_count = {sep: sample.count(sep) for sep in separators}
        likely_sep = max(sep_count, key=sep_count.get)
        
        logger.info(f"S√©parateur d√©tect√©: '{likely_sep}' (occurrences: {sep_count[likely_sep]})")
        
        # D√©tection intelligente de l'encodage
        encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'windows-1252']
        encoding_used = None
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.readline()
                encoding_used = encoding
                break
            except UnicodeDecodeError:
                continue
        
        if not encoding_used:
            logger.warning("Impossible de d√©terminer l'encodage, utilisation de utf-8 avec errors='ignore'")
            encoding_used = 'utf-8'
        
        logger.info(f"Encodage utilis√©: {encoding_used}")
        
        # Pour les fichiers volumineux, limiter le nombre de lignes ou utiliser chunks
        if file_size_mb > 500 and max_rows is None:

        logger.warning(f"Le fichier est tr√®s volumineux ({file_size_mb:.2f} MB), limitation √† 1 million de lignes")
            max_rows = 1000000
        
        # Si le fichier est immense, utiliser un traitement par morceaux
        if file_size_mb > 1000:  # Plus de 1 GB
            logger.info("Fichier extr√™mement volumineux, traitement par chunks")
            
            # Lecture et traitement similaire √† la version pr√©c√©dente
            # Code pour le traitement de gros fichiers...
            
        else:
            # Lecture normale pour les fichiers de taille raisonnable
            logger.info(f"Lecture du fichier avec pandas")
            
            try:
                df = pd.read_csv(file_path, sep=likely_sep, encoding=encoding_used, 
                                nrows=max_rows, error_bad_lines=False, warn_bad_lines=True,
                                low_memory=False)
            except Exception as e:
                logger.error(f"Erreur lors de la lecture avec le s√©parateur '{likely_sep}': {str(e)}")
                logger.info("Tentative avec le moteur Python et d√©tection automatique du s√©parateur")
                df = pd.read_csv(file_path, sep=None, encoding=encoding_used, 
                                nrows=max_rows, error_bad_lines=False, warn_bad_lines=True,
                                engine='python', low_memory=False)
            
            # Optimiser la m√©moire du DataFrame
            df = optimize_dataframe(df)
            
            # Informations de base
            row_count = len(df)
            col_count = len(df.columns)
            logger.info(f"Dimensions: {row_count} lignes, {col_count} colonnes")
        
        # Analyse statistique et visualisations (fonction d√©finie ailleurs)
        data_analysis = analyze_and_visualize_data(df, output_prefix, file_path)
        
        # Le reste du traitement et sauvegarde...
        
        return {
            "fichier": str(file_path),
            "lignes": row_count,
            "colonnes": col_count,
            "taille_mb": file_size_mb,
            "colonnes_numeriques": data_analysis["colonnes_numeriques"],
            "fichiers_sortie": [],
            "visualisations": data_analysis["visualisations"],
            "correlations": data_analysis["correlations"],
            "valeurs_manquantes": data_analysis["valeurs_manquantes"],
            "stats": data_analysis["stats"]
        }
            
    except Exception as e:
        logger.error(f"Erreur lors du traitement du fichier CSV {file_path}: {str(e)}", exc_info=True)
        return {"erreur": str(e)}

def main():
    """Fonction principale."""
    logger.info("=== D√©but du traitement avanc√© des donn√©es ===")
    
    # R√©cup√©rer les fichiers √† traiter
    files_to_process = find_files_to_process()
    
    if not files_to_process:
        logger.warning("Aucun fichier √† traiter trouv√©.")
        return 1
    
    results = []
    
    # Traiter chaque fichier
    for prefix, file_path in files_to_process.items():
        # D√©terminer la taille du fichier pour ajuster le traitement
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        
        # Limiter le nombre de lignes pour les fichiers volumineux
        max_rows = None
        if file_size_mb > 200:
            max_rows = 100000
            logger.info(f"Fichier volumineux ({file_size_mb:.2f} MB), limitation √† {max_rows} lignes")
        
        # Traiter le fichier
        result = process_csv(file_path, prefix, max_rows)
        results.append(result)
    
    # G√©n√©rer le rapport avanc√©
    report_file = generate_enhanced_report(results)
    
    if report_file:
        logger.info(f"Rapport avanc√© g√©n√©r√© avec succ√®s: {report_file}")
    else:
        logger.error("√âchec de la g√©n√©ration du rapport avanc√©")
    
    logger.info("=== Fin du traitement avanc√© des donn√©es ===")
    
    return 0 if report_file else 1

if __name__ == "__main__":
    sys.exit(main())
EOF
    
    chmod +x "$PROCESSING_SCRIPT"
    echo "‚úÖ Script de traitement avanc√© cr√©√©: $PROCESSING_SCRIPT" | tee -a "$LOG_FILE"
    return 0
}

# Fonction pour envoyer une notification Discord avec rapport et graphique
notify_discord_with_charts() {
    local title="$1"
    local message="$2"
    local report_path="$3"
    
    echo "üéÆ Tentative d'envoi de notification Discord avec rapport et graphique: $title" | tee -a "$LOG_FILE"
    
    # Cr√©er le script Discord s'il n'existe pas d√©j√†
    if [ ! -f "$SCRIPT_DIR/send_discord_with_charts.py" ]; then
        create_discord_charts_script
    fi
    
    # Rechercher un graphique √† inclure
    CHART_PATH=""
    if [ -d "$VISUALIZATION_DIR" ]; then
        # Prioriser les tableaux de bord
        CHART_PATH=$(find "$VISUALIZATION_DIR" -name "*_dashboard.png" -mtime -1 | head -1)
        
        # Si aucun tableau de bord, chercher n'importe quelle visualisation
        if [ -z "$CHART_PATH" ]; then
            CHART_PATH=$(find "$VISUALIZATION_DIR" -name "*.png" -mtime -1 | head -1)
        fi
        
        if [ -n "$CHART_PATH" ]; then
            echo "üîç Visualisation trouv√©e pour l'envoi: $CHART_PATH" | tee -a "$LOG_FILE"
        else
            echo "‚ö†Ô∏è Aucune visualisation r√©cente trouv√©e" | tee -a "$LOG_FILE"
        fi
    fi
    
    # Envoyer la notification via Discord avec le rapport et le graphique
    if python3 "$SCRIPT_DIR/send_discord_with_charts.py" "$DISCORD_WEBHOOK" "$message" "$title" "$report_path" "$CHART_PATH"; then
        echo "‚úÖ Notification Discord avec rapport et visualisation envoy√©e avec succ√®s" | tee -a "$LOG_FILE"
        return 0
    else
        echo "‚ùå √âchec de l'envoi de la notification Discord" | tee -a "$LOG_FILE"
        return 1
    fi
}

handle_error() {
    local step="$1"
    local error_msg="$2"
    
    show_figlet "ERROR"
    echo "‚ùå ERREUR √† l'√©tape '$step': $error_msg" | tee -a "$LOG_FILE"
    notify_discord_with_charts "‚ùå Erreur processus de donn√©es - √âtape: $step" "Le processus a √©chou√© √† l'√©tape '$step': $error_msg. Voir $LOG_FILE pour plus de d√©tails." ""
    exit 1
}

# === CONFIGURATION GIT ===
setup_git() {
    show_figlet "Git Setup"
    echo "--- Configuration du versionning Git ---" | tee -a "$LOG_FILE"
    
    if [ "$GIT_ENABLED" = true ]; then
        # V√©rifier si le r√©pertoire est un d√©p√¥t Git
        if [ ! -d ".git" ]; then
            echo "Initialisation du d√©p√¥t Git..." | tee -a "$LOG_FILE"
            git init || handle_error "Git init" "Impossible d'initialiser le d√©p√¥t Git"
        fi
        
        # V√©rifier s'il y a des modifications non commit√©es
        if git diff --quiet; then
            echo "‚úÖ Aucune modification non commit√©e d√©tect√©e" | tee -a "$LOG_FILE"
        else
            echo "‚ö†Ô∏è Modifications non commit√©es d√©tect√©es" | tee -a "$LOG_FILE"
            # Auto-commit des modifications actuelles avant de changer de branche
            git add . || handle_error "Git add" "Impossible d'ajouter les modifications actuelles"
            git commit -m "Auto-commit avant traitement de donn√©es: $DATE" || handle_error "Git commit" "Impossible de committer les modifications actuelles"
            echo "‚úÖ Modifications actuelles commit√©es" | tee -a "$LOG_FILE"
        fi
        
        # Cr√©er ou utiliser la branche de donn√©es
        if git show-ref --verify --quiet "refs/heads/$GIT_BRANCH"; then
            git checkout "$GIT_BRANCH" || handle_error "Git checkout" "Impossible de passer √† la branche $GIT_BRANCH"
            echo "‚úÖ Passage √† la branche existante: $GIT_BRANCH" | tee -a "$LOG_FILE"
        else
            git checkout -b "$GIT_BRANCH" || handle_error "Git branch" "Impossible de cr√©er la branche $GIT_BRANCH"
            echo "‚úÖ Cr√©ation et passage √† la nouvelle branche: $GIT_BRANCH" | tee -a "$LOG_FILE"
        fi
    else
        echo "‚ö†Ô∏è Git non disponible, √©tape de versionning ignor√©e" | tee -a "$LOG_FILE"
    fi
}

# === 1. T√âL√âCHARGEMENT DES DONN√âES MASSIVES ===
download_massive_data() {
    show_figlet "Big Data Download"
    echo "=== √âTAPE 1: T√âL√âCHARGEMENT DE DONN√âES MASSIVES (environ 250MB) ===" | tee -a "$LOG_FILE"
    
    # Sources de donn√©es volumineuses - environ 250+ MB combin√©es
    # Format: NOM|URL|TYPE_FICHIER|TAILLE_ESTIM√âE_MB
    SOURCES=(
      # Donn√©es m√©t√©orologiques compl√®tes
      "METEO_HISTORIQUE|https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/donnees-synop-essentielles-omm/exports/csv?limit=100000|CSV|60"
      
      # Donn√©es √©conomiques de l'INSEE
      "INSEE_ECONOMIE|https://www.insee.fr/fr/statistiques/fichier/6544344/base-cc-emploi-pop-act-2019-csv.zip|ZIP|45"
      
      # Donn√©es environnementales
      "ENVIRO_EU|https://www.eea.europa.eu/data-and-maps/data/waterbase-water-quality-icm-2/waterbase-water-quality-icm-2/waterbase-water-quality-data-results.csv/at_download/file|CSV|55"
      
      # Donn√©es de transport
      "SNCF_DATA|https://ressources.data.sncf.com/api/v2/catalog/datasets/regularite-mensuelle-tgv-aqst/exports/csv?limit=-1&timezone=Europe%2FBerlin|CSV|35"
      
      # Donn√©es d√©mographiques
      "WORLD_POP|https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/CSV_FILES/WPP2022_Demographic_Indicators_Medium.zip|ZIP|45"
    )
    
    echo "üìä Volume total √† t√©l√©charger: environ 250MB en 5 sources" | tee -a "$LOG_FILE"
    
    # V√©rification d'espace disque
    AVAILABLE_SPACE=$(df -m "$RAW_DIR" | awk 'NR==2 {print $4}')
    echo "üíæ Espace disque disponible: ${AVAILABLE_SPACE}MB" | tee -a "$LOG_FILE"
    
    if [ "$AVAILABLE_SPACE" -lt 500 ]; then
        echo "‚ö†Ô∏è AVERTISSEMENT: L'espace disque disponible ($AVAILABLE_SPACE MB) est faible pour le t√©l√©chargement" | tee -a "$LOG_FILE"
        read -p "Voulez-vous continuer quand m√™me? (o/n) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Oo]$ ]]; then
            handle_error "T√©l√©chargement" "Espace disque insuffisant"
        fi
    fi
    
    SUCCESS_COUNT=0
    FAILED_COUNT=0
    
    for SOURCE in "${SOURCES[@]}"; do
        IFS='|' read -r NAME URL TYPE SIZE_MB <<< "$SOURCE"
        
        echo "üì• T√©l√©chargement de $NAME (${SIZE_MB}MB environ) depuis $URL" | tee -a "$LOG_FILE"
        
        OUTPUT_FILE="$RAW_DIR/${NAME}_${DATE}.${TYPE,,}"
        
        # T√©l√©charger avec timeout et retry
        if curl -L --retry 5 --retry-delay 10 --max-time 3600 -C - --progress-bar -o "$OUTPUT_FILE" "$URL"; then
            if [ -s "$OUTPUT_FILE" ]; then
                ACTUAL_SIZE=$(du -m "$OUTPUT_FILE" | cut -f1)
                echo "‚úÖ T√©l√©chargement r√©ussi: $OUTPUT_FILE (${ACTUAL_SIZE}MB)" | tee -a "$LOG_FILE"
                
                # Ajouter m√©tadonn√©es
                echo "# Donn√©es t√©l√©charg√©es le: $DATE √† $(date '+%H:%M:%S')" > "$OUTPUT_FILE.meta"
                echo "# Source: $URL" >> "$OUTPUT_FILE.meta"
                echo "# Type: $TYPE" >> "$OUTPUT_FILE.meta"
                echo "# Taille: ${ACTUAL_SIZE}MB" >> "$OUTPUT_FILE.meta"
                
                # Traitement sp√©cial pour les ZIP - d√©compression
                if [[ "${TYPE,,}" == "zip" ]]; then
                    echo "üì¶ D√©compression du fichier ZIP..." | tee -a "$LOG_FILE"
                    EXTRACT_DIR="$RAW_DIR/${NAME}_${DATE}_extracted"
                    mkdir -p "$EXTRACT_DIR"
                    
                    if unzip -q "$OUTPUT_FILE" -d "$EXTRACT_DIR"; then
                        echo "‚úÖ D√©compression r√©ussie dans $EXTRACT_DIR" | tee -a "$LOG_FILE"
                        
                        # Liste des fichiers extraits
                        echo "üìÑ Fichiers extraits:" | tee -a "$LOG_FILE"
                        find "$EXTRACT_DIR" -type f | tee -a "$LOG_FILE"
                    else
                        echo "‚ùå √âchec de la d√©compression de $OUTPUT_FILE" | tee -a "$LOG_FILE"
                    fi
                fi
                
                # Pour les CSV, afficher uniquement les 2 premi√®res lignes
                if [[ "${TYPE,,}" == "csv" ]]; then
                    echo "üìä Aper√ßu des donn√©es (2 premi√®res lignes):" | tee -a "$LOG_FILE"
                    head -n 2 "$OUTPUT_FILE" | tee -a "$LOG_FILE"
                    
                    # Nombre total de lignes
                    LINE_COUNT=$(wc -l < "$OUTPUT_FILE")
                    echo "üìè Nombre total de lignes: $LINE_COUNT" | tee -a "$LOG_FILE"
                fi
                
                # Calcul du hash pour les fichiers volumineux
                echo "üîê Calcul du hash SHA256..." | tee -a "$LOG_FILE"
                SHA=$(sha256sum "$OUTPUT_FILE" | cut -d' ' -f1)
                echo "$SHA" > "$OUTPUT_FILE.sha256"
                echo "üîê Hash SHA256: $SHA" | tee -a "$LOG_FILE"
                
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            else
                echo "‚ùå Fichier t√©l√©charg√© vide: $NAME" | tee -a "$LOG_FILE"
                rm -f "$OUTPUT_FILE"
                FAILED_COUNT=$((FAILED_COUNT + 1))
            fi
        else
            echo "‚ùå √âchec du t√©l√©chargement de $NAME" | tee -a "$LOG_FILE"
            FAILED_COUNT=$((FAILED_COUNT + 1))
        fi
        
        echo "------------------------------------------------" | tee -a "$LOG_FILE"
    done
    
    # R√©sum√© du t√©l√©chargement
    echo "===== R√âSUM√â DU T√âL√âCHARGEMENT =====" | tee -a "$LOG_FILE"
    echo "‚úÖ T√©l√©chargements r√©ussis: $SUCCESS_COUNT" | tee -a "$LOG_FILE"
    echo "‚ùå T√©l√©chargements √©chou√©s: $FAILED_COUNT" | tee -a "$LOG_FILE"
    echo "üíæ Espace disque utilis√©: $(du -sh "$RAW_DIR" | cut -f1)" | tee -a "$LOG_FILE"
    
    # V√©rifier si au moins une source a √©t√© t√©l√©charg√©e
    if [ "$SUCCESS_COUNT" -eq 0 ]; then
        handle_error "T√©l√©chargement" "Aucune source n'a pu √™tre t√©l√©charg√©e"
    else
        echo "‚úÖ T√©l√©chargement de $SUCCESS_COUNT/$((SUCCESS_COUNT + FAILED_COUNT)) sources termin√© avec succ√®s" | tee -a "$LOG_FILE"
    fi
}

# === 2. TRAITEMENT AVANC√â DES DONN√âES ===
process_massive_data() {
    show_figlet "Advanced Processing"
    echo "=== √âTAPE 2: TRAITEMENT AVANC√â DES DONN√âES MASSIVES ===" | tee -a "$LOG_FILE"
    
    echo "üîÑ Lancement du script de traitement Python avanc√©..." | tee -a "$LOG_FILE"
    
    # Cr√©er le script de traitement s'il n'existe pas d√©j√†
    if [ ! -f "$SCRIPT_DIR/process_data_advanced.py" ]; then
        create_processing_script
    fi
    
    # Ex√©cuter le script
    python3 "$SCRIPT_DIR/process_data_advanced.py"
    PYTHON_EXIT_CODE=$?
    
    # V√©rifier le r√©sultat
    if [ $PYTHON_EXIT_CODE -ne 0 ]; then
        handle_error "Traitement" "Le script Python avanc√© a √©chou√© avec le code d'erreur $PYTHON_EXIT_CODE"
    else
        echo "‚úÖ Traitement avanc√© des donn√©es termin√© avec succ√®s" | tee -a "$LOG_FILE"
    fi
}

# === 3. G√âN√âRATION DE RAPPORT AM√âLIOR√â ===
generate_enhanced_report() {
    show_figlet "Enhanced Report"
    echo "=== √âTAPE 3: G√âN√âRATION DU RAPPORT AVANC√â ===" | tee -a "$LOG_FILE"
    
    # V√©rifier si le rapport a √©t√© g√©n√©r√© par le script Python
    REPORT_HTML=$(find "$REPORT_DIR" -name "rapport_advanced_$DATE.html")
    
    if [ -n "$REPORT_HTML" ]; then
        echo "‚úÖ Rapport avanc√© g√©n√©r√© avec succ√®s: $REPORT_HTML" | tee -a "$LOG_FILE"
    else
        echo "‚ö†Ô∏è Rapport avanc√© non trouv√©, recherche d'un rapport standard..." | tee -a "$LOG_FILE"
        REPORT_HTML=$(find "$REPORT_DIR" -name "rapport_$DATE.html")
        
        if [ -n "$REPORT_HTML" ]; then
            echo "‚úÖ Rapport standard trouv√©: $REPORT_HTML" | tee -a "$LOG_FILE"
        else
            handle_error "Rapport" "Aucun rapport n'a √©t√© g√©n√©r√©"
        fi
    fi
    
    # Compter les visualisations g√©n√©r√©es
    VIZ_COUNT=$(find "$VISUALIZATION_DIR" -type f -name "*.png" -mtime -1 | wc -l)
    echo "üìä Nombre de visualisations g√©n√©r√©es: $VIZ_COUNT" | tee -a "$LOG_FILE"
    
    # Cr√©er un r√©sum√© du rapport
    SUMMARY=$(cat << EOF
==========================================================
           R√âSUM√â DU TRAITEMENT AVANC√â DE DONN√âES
==========================================================
üìÖ Date d'ex√©cution: $DATE √† $(date '+%H:%M:%S')
üë§ Ex√©cut√© par: $(whoami)

üìä STATISTIQUES:
----------------------------------------------------------
üì• Donn√©es massives trait√©es:       ~250MB
üìà Visualisations g√©n√©r√©es:         $VIZ_COUNT
üìÑ Rapport avanc√©:                  $REPORT_HTML

üìÇ EMPLACEMENTS:
----------------------------------------------------------
üìä Donn√©es brutes:                  $RAW_DIR
üìà Donn√©es trait√©es:                $PROCESSED_DIR
üìë Rapports et visualisations:      $REPORT_DIR
üìù Logs:                            $LOG_DIR

Pour consulter le rapport complet, ouvrez:
$REPORT_HTML
==========================================================
EOF
)
    
    echo "$SUMMARY" | tee -a "$LOG_FILE"
    
    # Enregistrer le r√©sum√© dans un fichier
    SUMMARY_FILE="$REPORT_DIR/resume_avance_$DATE.txt"
    echo "$SUMMARY" > "$SUMMARY_FILE"
    
    echo "‚úÖ R√©sum√© avanc√© g√©n√©r√© et enregistr√© dans $SUMMARY_FILE" | tee -a "$LOG_FILE"
    
    return 0
}

# === 4. VERSIONNING GIT ===
commit_to_git() {
    show_figlet "Git Update"
    echo "=== √âTAPE 4: VERSIONNING GIT DES DONN√âES MASSIVES ===" | tee -a "$LOG_FILE"
    
    if [ "$GIT_ENABLED" = true ]; then
        echo "üì¶ Ajout des fichiers au suivi Git (sans les donn√©es brutes volumineuses)" | tee -a "$LOG_FILE"
        
        # Ajouter les fichiers au suivi Git, en excluant les fichiers volumineux
        git add "$LOG_DIR" 
        git add "$DATA_DIR/processed"
        git add "$DATA_DIR/reports"
        git add "$DATA_DIR/visualizations"
        git add "$DATA_DIR/raw/*.meta" 
        git add "$DATA_DIR/raw/*.sha256"
        
        # V√©rifier s'il y a des changements √† committer
        if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è Aucun changement √† committer" | tee -a "$LOG_FILE"
        else
            echo "üíæ Commit des changements" | tee -a "$LOG_FILE"
            git commit -m "Traitement de donn√©es massives: $DATE - ~250MB trait√©es" || {
                echo "‚ö†Ô∏è Erreur lors du commit Git" | tee -a "$LOG_FILE"
            }
            
            # Push vers le d√©p√¥t distant (si configur√©)
            if git remote | grep -q "origin"; then
                echo "üîÑ Push des changements vers le d√©p√¥t distant" | tee -a "$LOG_FILE"
                git push origin "$GIT_BRANCH" || {
                    echo "‚ö†Ô∏è √âchec du push vers origin" | tee -a "$LOG_FILE"
                }
            else
                echo "‚ÑπÔ∏è Aucun d√©p√¥t distant configur√©" | tee -a "$LOG_FILE"
            fi
        fi
    else
        echo "‚ö†Ô∏è Git non disponible, √©tape de versionning ignor√©e" | tee -a "$LOG_FILE"
    fi
}

# === 5. NOTIFICATION AVEC VISUALISATIONS ===
send_enhanced_notification() {
    show_figlet "Enhanced Notify"
    echo "=== √âTAPE 5: ENVOI DE NOTIFICATION AM√âLIOR√âE AVEC VISUALISATIONS ===" | tee -a "$LOG_FILE"
    
    # Trouver le rapport g√©n√©r√©
    REPORT_HTML=$(find "$REPORT_DIR" -name "rapport_advanced_$DATE.html" -o -name "rapport_$DATE.html" | head -1)
    
    # Trouver les visualisations les plus int√©ressantes
    DASHBOARD=$(find "$VISUALIZATION_DIR" -name "*_dashboard.png" -mtime -1 | head -1)
    
    # Compter les √©l√©ments g√©n√©r√©s
    VIZ_COUNT=$(find "$VISUALIZATION_DIR" -type f -name "*.png" -mtime -1 | wc -l)
    DATA_SIZE=$(du -sh "$RAW_DIR" | cut -f1)
    
    # Cr√©er le message de notification
    NOTIFICATION="
üöÄ **Traitement de donn√©es massives termin√© avec succ√®s**

üìä **R√©sum√© du traitement:**
- üì• Volume trait√©: ~250MB
- üßπ Donn√©es nettoy√©es et optimis√©es
- üìà $VIZ_COUNT visualisations g√©n√©r√©es
- üìä Analyses statistiques avanc√©es r√©alis√©es

Le rapport complet avec toutes les visualisations est disponible √†: $REPORT_HTML

Ce message inclut l'une des visualisations g√©n√©r√©es automatiquement √† partir des donn√©es.
"
    
    # Envoyer la notification Discord avec le rapport HTML et le graphique
    NOTIFICATION_TITLE="‚úÖ Traitement avanc√© de donn√©es massives r√©ussi - $DATE"
    notify_discord_with_charts "$NOTIFICATION_TITLE" "$NOTIFICATION" "$REPORT_HTML"
    
    echo "üéÆ Notification avanc√©e avec visualisations envoy√©e" | tee -a "$LOG_FILE"
}

# === EX√âCUTION PRINCIPALE ===
main() {
    # √âtape 0: Configuration Git (si disponible)
    setup_git
    
    # √âtape 1: T√©l√©charger les donn√©es massives
    download_massive_data
    
    # √âtape 2: Traiter les donn√©es massives
    process_massive_data
    
    # √âtape 3: G√©n√©rer le rapport avanc√©
    generate_enhanced_report
    
    # √âtape 4: Versionning Git
    commit_to_git
    
    # √âtape 5: Envoyer la notification avanc√©e
    send_enhanced_notification
    
    # Terminer
    show_figlet "Success"
    echo "===== FIN DU PROCESSUS AVANC√â: $(date '+%Y-%m-%d %H:%M:%S') =====" | tee -a "$LOG_FILE"
    echo "‚úÖ Processus de traitement de donn√©es massives termin√© avec succ√®s"
    echo "üìÑ Pour plus de d√©tails, consultez les logs: $LOG_FILE"
    echo "üìä Rapport HTML avanc√©: $(find "$REPORT_DIR" -name "rapport_advanced_$DATE.html")"
    echo "üåê Taille totale du jeu de donn√©es: $(du -sh "$DATA_DIR" | cut -f1)"
    
    return 0
}

# Lancer l'ex√©cution principale
main